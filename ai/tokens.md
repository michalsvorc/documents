# Tokens

Tokens are segments of text that the model reads and generates. They can be as short as a single character or as long as a whole word.

## Estimating Token Usage

Approximate Conversions:
  - 1 token ≈ 4 characters in English
  - 1 token ≈ ¾ of a word
  - 100 tokens ≈ 75 words
  - 1-2 sentences ≈ 30 tokens
  - 1 paragraph ≈ 100 tokens
  - 1,500 words ≈ 2,048 tokens
